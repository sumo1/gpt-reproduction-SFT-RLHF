{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# ğŸ“Œ 1. å¯¼å…¥åº“\n",
    "import torch\n",
    "from transformers import (\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  #è¿™é‡Œä½¿ç”¨äº†colabä¸­çš„google driveæ¥ä¿å­˜ä¿¡æ¯åˆ°äº‘ç›˜ã€‚\n",
    "\n",
    "# ğŸ“Œ 2. æ£€æŸ¥ GPU\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "\n",
    "# ğŸ“Œ 3. åŠ è½½ wikitext æ•°æ®é›†\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# ğŸ“Œ 4. åŠ è½½ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # å¿…é¡»è®¾ç½® pad_token\n",
    "\n",
    "# ğŸ“Œ 5. Tokenize & æ‹¼æ¥æ•°æ®ä¸ºé•¿åºåˆ—\n",
    "block_size = 128\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"])\n",
    "\n",
    "def group_texts(examples):\n",
    "    # æ‹¼æ¥æ‰€æœ‰ token\n",
    "    joined = sum(examples[\"input_ids\"], [])\n",
    "    total_length = len(joined)\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        \"input_ids\": [joined[i:i+block_size] for i in range(0, total_length, block_size)],\n",
    "        \"attention_mask\": [ [1]*block_size ] * (total_length // block_size)\n",
    "    }\n",
    "    return result\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "lm_dataset = tokenized.map(group_texts, batched=True)\n",
    "\n",
    "# ğŸ“Œ 6. æ„å»ºæ¨¡å‹\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# ğŸ“Œ 7. è®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/models/gpt1-wikitext-model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_dir=\"/content/drive/MyDrive/models/logs\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True  # âœ… æ··åˆç²¾åº¦\n",
    ")\n",
    "\n",
    "# ğŸ“Œ 8. è®­ç»ƒå™¨å‡†å¤‡\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# ğŸ“Œ 9. å¼€å§‹è®­ç»ƒï¼\n",
    "trainer.train()\n",
    "\n",
    "# ğŸ“Œ 10. ä¿å­˜æ¨¡å‹\n",
    "trainer.save_model(\"/content/drive/MyDrive/models/gpt1-wikitext-final\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/models/gpt1-wikitext-final\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# æŸ¥çœ‹è®­ç»ƒæ›²çº¿\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logs = trainer.state.log_history\n",
    "steps = [log[\"step\"] for log in logs if \"loss\" in log]\n",
    "losses = [log[\"loss\"] for log in logs if \"loss\" in log]\n",
    "\n",
    "plt.plot(steps, losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"GPT-1 Training Loss\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "854f2f8c7c086acf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# æŸ¥è¯¢è®­ç»ƒæ•ˆæœ\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, pipeline\n",
    "\n",
    "# åŠ è½½ä½ è®­ç»ƒå¥½çš„æ¨¡å‹å’Œ tokenizer\n",
    "model_path = \"/content/drive/MyDrive/models/gpt1-wikitext-final\"  # ä½ å®é™…ä¿å­˜çš„ä½ç½®\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.to(\"cuda\").eval()  # åŠ ä¸Š eval() å’Œ CUDA\n",
    "\n",
    "# åˆ›å»ºç”Ÿæˆ pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# è¯•è¯•ä½ çš„ prompt\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "outputs = generator(\n",
    "    prompt,\n",
    "    max_length=100,          # æœ€é•¿ç”Ÿæˆé•¿åº¦\n",
    "    do_sample=True,          # ä½¿ç”¨é‡‡æ ·\n",
    "    top_k=50,                # é‡‡æ · top-k\n",
    "    top_p=0.95,              # nucleus sampling\n",
    "    temperature=1.0,         # æ§åˆ¶å¤šæ ·æ€§\n",
    "    num_return_sequences=1   # è¿”å›ä¸€ä¸ªç»“æœ\n",
    ")\n",
    "\n",
    "print(\"=== Generated Text ===\")\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "id": "2b0fc82b44175cf0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ç»“æœè¾“å‡ºï¼š\n",
    "=== Generated Text ===\n",
    "\n",
    "The future of artificial intelligence on the United States, and the entire city was the first first series. He was built by the game at $ 6 in the 19th World War after the 2006, the series was released in the 19 â€“ 1 January 2007. When the same year he had first year as well as well as part of all @-@ single as a result of the New York, although the season was played in September 2009. It was released, and the song was established by the year"
   ],
   "id": "20fee1d9b3030c62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# åŸºäºqs_sft_zh.jsonlçš„æ•°æ®è¿›è¡ŒSFT\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# è·¯å¾„\n",
    "data_path = \"/content/drive/MyDrive/data/qs_sft_zh.jsonl\"\n",
    "model_path = \"/content/drive/MyDrive/models/gpt1-wikitext-final\"\n",
    "save_path = \"/content/drive/MyDrive/models/gpt1-sft-qs\"\n",
    "\n",
    "# ğŸ“Œ 1. åŠ è½½åŸå§‹ JSONL æ•°æ®å¹¶åˆ‡åˆ†éªŒè¯é›†\n",
    "dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# ğŸ“Œ 2. åŠ è½½ tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ğŸ“Œ 3. Tokenize æ•°æ®\n",
    "def format_prompt(example):\n",
    "    full_text = f\"é—®ï¼š{example['prompt']}\\nç­”ï¼š{example['response']}\"\n",
    "    return {\"text\": full_text}\n",
    "\n",
    "train_dataset = train_dataset.map(format_prompt)\n",
    "eval_dataset = eval_dataset.map(format_prompt)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"])\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=[\"prompt\", \"response\", \"text\"])\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True, remove_columns=[\"prompt\", \"response\", \"text\"])\n",
    "\n",
    "\n",
    "# ğŸ“Œ 4. åŠ è½½æ¨¡å‹\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# ğŸ“Œ 5. è®­ç»ƒå‚æ•°ï¼ˆå¼€å¯éªŒè¯ï¼‰\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"steps\",   # âœ… æ¯ N æ­¥è¯„ä¼°ä¸€æ¬¡\n",
    "    eval_steps=100,\n",
    "    logging_dir=f\"{save_path}/logs\",\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# ğŸ“Œ 6. Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# ğŸ“Œ 7. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # âœ… æ·»åŠ éªŒè¯é›†\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ğŸ“Œ 8. å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()\n",
    "\n",
    "# ğŸ“Œ 9. ä¿å­˜æ¨¡å‹\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ],
   "id": "d86335553402ff66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# éªŒè¯å¾®è°ƒç»“æœ\n",
    "import torch\n",
    "\n",
    "def ask_model(question, max_new_tokens=128):\n",
    "    prompt = f\"ç”¨æˆ·ï¼š{question}\\nåŠ©æ‰‹ï¼š\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False  # greedy decoding\n",
    "        )\n",
    "\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = full_output.split(\"åŠ©æ‰‹ï¼š\", 1)[-1].strip()\n",
    "    return answer\n",
    "\n",
    "print(ask_model(\"2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åä¸­äºšæ´²å¤§å­¦è¡¨ç°å¦‚ä½•ï¼Ÿ\"))\n",
    "print(ask_model(\"QS å’Œ THE æ’åçš„ä¸»è¦åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ\"))\n",
    "print(ask_model(\"å¦‚ä½•ä½¿ç”¨ QS æ’åé€‰æ‹©é€‚åˆè‡ªå·±çš„å­¦æ ¡ï¼Ÿ\"))"
   ],
   "id": "fdf422fd310ca768"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# è¾“å‡º\n",
    "æ’åä¸­ï¼Œæ¸…åå¤§å­¦ç•¥é«˜äºåŒ—äº¬å¤§å­¦ï¼Œåˆ†åˆ—å…¨çƒç¬¬ 17 å’Œç¬¬ 18 ä½ï¼Œä¸¤è€…å·®è·éå¸¸å°ï¼Œå‡ä¸ºäºšæ´²é¡¶å°–é«˜æ ¡ã€‚æºï¼Œå¾—åˆ†è¶Šé«˜ã€‚ï¿½ä¾¿äºä¸œäº¬å¤§å­¦ä¹Ÿ\n",
    "QS æ˜¯ä¸¤ä¸ªè‘—åçš„å¤§å­¦æ’åæœºæ„ã€‚QS æ›´é‡è§†å­¦æœ¯å£°èª‰å’Œé›‡ä¸»å£°èª‰ï¼Œè€Œ THE æ›´å…³æ³¨æ•™å­¦ã€ç ”ç©¶å’Œå›½é™…è§†é‡ç­‰æ–¹é¢çš„ç»¼åˆè¡¨ç°ã€‚ä¸¤è€…æƒé‡ä½“ç³»ä¸ï¿½\n",
    "é€®å¦‚æŒ‡æ ‡æ˜¯é€šè¿‡ç»Ÿè®¡ä½ï¼Œè¿˜åº”å…³æ³¨å­¦ç§‘æ’åã€åœ°ç†ä½ç½®ã€å­¦è´¹ã€è¯­è¨€ç¯å¢ƒå’Œä¸ªäººå…´è¶£ç­‰å› ç´ ã€‚QS æä¾›äº†è¯¦ç»†çš„åˆ†é¡¹æ•°æ®ï¼Œä¾¿äºä¸ªï¿½"
   ],
   "id": "1fef175f6071e7d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# å‡†å¤‡SFTä¹‹åçš„å¼ºåŒ–å­¦ä¹ ï¼Œæå‡æ€§èƒ½ï¼ŒåŸºäºRLHF\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from transformers import AutoTokenizer, GPT2Model, GPT2Config, get_scheduler\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# === 1. å‚æ•°è®¾ç½® ===\n",
    "model_name_or_path = \"/content/drive/MyDrive/models/gpt1-sft-qs\"\n",
    "data_path = \"/content/drive/MyDrive/data/qs_ranking_rm_data.jsonl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_length = 512\n",
    "batch_size = 2\n",
    "num_epochs = 8\n",
    "lr = 5e-5\n",
    "\n",
    "# === 2. æ•°æ®é›† ===\n",
    "class PairwiseRMData(Dataset):\n",
    "    def __init__(self, path, tokenizer):\n",
    "        self.data = []\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                #print(line)\n",
    "                item = json.loads(line)\n",
    "                self.data.append(item)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.data[idx]\n",
    "        prompt = d[\"prompt\"]\n",
    "        chosen = prompt + d[\"chosen\"]\n",
    "        rejected = prompt + d[\"rejected\"]\n",
    "        chosen_ids = self.tokenizer(chosen, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "        rejected_ids = self.tokenizer(rejected, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "        return {\n",
    "            \"chosen_input_ids\": chosen_ids[\"input_ids\"].squeeze(0),\n",
    "            \"chosen_attention_mask\": chosen_ids[\"attention_mask\"].squeeze(0),\n",
    "            \"rejected_input_ids\": rejected_ids[\"input_ids\"].squeeze(0),\n",
    "            \"rejected_attention_mask\": rejected_ids[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# === 3. æ¨¡å‹ç»“æ„ ===\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path):\n",
    "        super().__init__()\n",
    "        config = GPT2Config.from_pretrained(model_name_or_path)\n",
    "        config.pad_token_id = config.eos_token_id\n",
    "        self.gpt = GPT2Model.from_pretrained(model_name_or_path, config=config)\n",
    "        self.value_head = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs.last_hidden_state  # [B, L, H]\n",
    "        rewards = self.value_head(last_hidden).squeeze(-1)  # [B, L]\n",
    "        lengths = attention_mask.sum(dim=1) - 1\n",
    "        last_rewards = rewards[range(rewards.size(0)), lengths]\n",
    "        return last_rewards\n",
    "\n",
    "# === 4. åˆå§‹åŒ– ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = PairwiseRMData(data_path, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = RewardModel(model_name_or_path).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * num_epochs)\n",
    "\n",
    "# === 5. è®­ç»ƒå¾ªç¯ ===\n",
    "loss_fn = nn.MarginRankingLoss(margin=0.5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch in pbar:\n",
    "        chosen_ids = batch[\"chosen_input_ids\"].to(device)\n",
    "        chosen_mask = batch[\"chosen_attention_mask\"].to(device)\n",
    "        rejected_ids = batch[\"rejected_input_ids\"].to(device)\n",
    "        rejected_mask = batch[\"rejected_attention_mask\"].to(device)\n",
    "\n",
    "        chosen_rewards = model(chosen_ids, chosen_mask)\n",
    "        rejected_rewards = model(rejected_ids, rejected_mask)\n",
    "\n",
    "        loss = loss_fn(chosen_rewards, rejected_rewards, torch.ones_like(chosen_rewards).to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "# === 6. ä¿å­˜æ¨¡å‹ ===\n",
    "save_path = \"/content/drive/MyDrive/models/rm-qs\"\n",
    "model.gpt.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"âœ… å¥–åŠ±æ¨¡å‹ä¿å­˜å®Œæˆï¼\")"
   ],
   "id": "4981325207cad446"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# è¾“å‡º\n",
    "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:32<00:00,  4.30it/s, loss=0]\n",
    "\n",
    "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:32<00:00,  4.33it/s, loss=0]\n",
    "\n",
    "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:32<00:00,  4.35it/s, loss=0]\n",
    "\n",
    "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:32<00:00,  4.34it/s, loss=0]\n",
    "\n",
    "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:32<00:00,  4.33it/s, loss=0]\n",
    "\n",
    "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:32<00:00,  4.34it/s, loss=0]\n",
    "\n",
    "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:32<00:00,  4.34it/s, loss=0]\n",
    "\n",
    "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140/140 [00:32<00:00,  4.34it/s, loss=0]\n",
    "\n",
    "âœ… å¥–åŠ±æ¨¡å‹ä¿å­˜å®Œæˆï¼"
   ],
   "id": "4703705ff7f42072"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å’Œ tokenizer\n",
    "reward_model_path = \"/content/drive/MyDrive/models/rm-qs\"\n",
    "\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(reward_model_path)\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained(reward_model_path)\n",
    "rm_model.eval().cuda()  # å¦‚æœæœ‰ GPU çš„è¯\n",
    "\n",
    "# è¾“å…¥æµ‹è¯•å¯¹\n",
    "prompt = \"è¯·ä»‹ç»æ¸…åå¤§å­¦ã€‚\"\n",
    "response = \"æ¸…åå¤§å­¦æ˜¯ä¸­å›½é¡¶å°–çš„ç»¼åˆæ€§å¤§å­¦ï¼Œä½äºåŒ—äº¬å¸‚ã€‚\"\n",
    "\n",
    "# æ„é€ æ¨¡å‹è¾“å…¥\n",
    "text = f\"Prompt: {prompt}\\nResponse: {response}\"  # ç¡®ä¿æ ¼å¼å’Œè®­ç»ƒæ—¶ä¸€è‡´\n",
    "inputs = rm_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(rm_model.device)\n",
    "\n",
    "# å‰å‘æ¨ç†å¾—åˆ° reward åˆ†æ•°\n",
    "logits = rm_model(**inputs).logits\n",
    "reward = logits.squeeze()\n",
    "if reward.numel() > 1:\n",
    "    reward = reward[0]\n",
    "print(\"Reward score:\", reward.item())"
   ],
   "id": "5b1c3dd9c45407f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# è¾“å‡º\n",
    "Reward score: 0.35942158102989197\n",
    "\n"
   ],
   "id": "bf25d8c12dbe926c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# åŸºäºä¸Šé¢çš„SFTå’Œå¥–åŠ±æ¨¡å‹è¿›è¡ŒPPO RLHF\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, GPT2Model, GPT2Config\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torch import nn\n",
    "\n",
    "# =============================\n",
    "# 1. è·¯å¾„è®¾ç½®\n",
    "# =============================\n",
    "sft_model_path = \"/content/drive/MyDrive/models/gpt1-sft-qs\"\n",
    "reward_model_path = \"/content/drive/MyDrive/models/rm-qs\"\n",
    "save_path = \"/content/drive/MyDrive/models/gpt1-rlhf-qs\"\n",
    "prompts_file = \"/content/drive/MyDrive/data/qs_sft_zh.jsonl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 2. è‡ªå®šä¹‰ Dataset\n",
    "# =============================\n",
    "class PromptOnlyDataset(TorchDataset):\n",
    "    def __init__(self, jsonl_path):\n",
    "        self.data = []\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                self.data.append(item[\"prompt\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"prompt\": self.data[idx]}\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 3. åŠ è½½æ¨¡å‹å’Œ tokenizer\n",
    "# =============================\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(sft_model_path).to(device)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(sft_model_path).to(device)\n",
    "ref_model.eval()\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 4. å®šä¹‰æ•°æ®é›†å’Œ collator\n",
    "# =============================\n",
    "prompts_dataset = PromptOnlyDataset(prompts_file)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    prompts = [item[\"prompt\"] for item in batch]\n",
    "    tokenized = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"].to(device)\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"].to(device)\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 5. PPO é…ç½®å’Œ Trainer\n",
    "# =============================\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=8,\n",
    "    mini_batch_size=4,\n",
    "    log_with=\"tensorboard\",\n",
    "    project_kwargs={\"logging_dir\": \"/content/drive/MyDrive/models/tensorboard_logs\"}\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=prompts_dataset,\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 6. åŠ è½½å¥–åŠ±æ¨¡å‹\n",
    "# =============================\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path):\n",
    "        super().__init__()\n",
    "        config = GPT2Config.from_pretrained(model_name_or_path)\n",
    "        config.pad_token_id = config.eos_token_id\n",
    "        self.gpt = GPT2Model.from_pretrained(model_name_or_path, config=config)\n",
    "        self.value_head = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs.last_hidden_state  # [B, L, H]\n",
    "        rewards = self.value_head(last_hidden).squeeze(-1)  # [B, L]\n",
    "        lengths = attention_mask.sum(dim=1) - 1\n",
    "        last_rewards = rewards[range(rewards.size(0)), lengths]\n",
    "        return last_rewards\n",
    "\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_path)\n",
    "reward_tokenizer.pad_token = reward_tokenizer.eos_token\n",
    "reward_model = RewardModel(reward_model_path).to(device)\n",
    "reward_model.eval()\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 7. å¥–åŠ±å‡½æ•°ï¼ˆè°ƒç”¨ reward modelï¼‰\n",
    "# =============================\n",
    "def reward_fn(responses, prompts):\n",
    "    texts = [p + r for p, r in zip(prompts, responses)]\n",
    "    tokenized = reward_tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        rewards = reward_model(\n",
    "            input_ids=tokenized[\"input_ids\"],\n",
    "            attention_mask=tokenized[\"attention_mask\"]\n",
    "        )\n",
    "    return list(rewards)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 8. PPO å¼ºåŒ–è®­ç»ƒä¸»å¾ªç¯\n",
    "# =============================\n",
    "num_rl_epochs = 10\n",
    "log_file = \"/content/drive/MyDrive/models/ppo_rlhf_log.txt\"\n",
    "\n",
    "print(\"Starting PPO RLHF training...\")\n",
    "\n",
    "for epoch in range(num_rl_epochs):\n",
    "    print(f\"=== RL Epoch {epoch+1}/{num_rl_epochs} ===\")\n",
    "\n",
    "    for step, batch in enumerate(tqdm(ppo_trainer.dataloader, desc=\"PPO Training\")):\n",
    "        queries = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        # === æ¨¡å‹ç”Ÿæˆå›å¤ ===\n",
    "        responses = model.generate(\n",
    "            input_ids=queries,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        query_texts = tokenizer.batch_decode(queries, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        response_texts = tokenizer.batch_decode(responses, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        # === å»é™¤é‡å¤ prompt ===\n",
    "        cleaned_responses = []\n",
    "        for q, r in zip(query_texts, response_texts):\n",
    "            if r.startswith(q):\n",
    "                cleaned_responses.append(r[len(q):].strip())\n",
    "            else:\n",
    "                cleaned_responses.append(r.strip())\n",
    "\n",
    "        # === è®¡ç®—å¥–åŠ± ===\n",
    "        rewards = reward_fn(cleaned_responses, query_texts)\n",
    "\n",
    "        # === Tokenize å†æ¬¡ç”¨äº PPOTrainer ===\n",
    "        query_ids = [q.to(device) for q in queries]\n",
    "        response_ids = [\n",
    "            tokenizer(r, return_tensors=\"pt\", truncation=True, max_length=256).input_ids[0].to(device)\n",
    "            for r in cleaned_responses\n",
    "        ]\n",
    "\n",
    "        stats = ppo_trainer.step(query_ids, response_ids, rewards)\n",
    "\n",
    "        if step % 1 == 0:\n",
    "            print(f\"\\nStep {step}\")\n",
    "            print(f\"Prompt: {query_texts[0][:50]}\")\n",
    "            print(f\"Response: {cleaned_responses[0][:100]}\")\n",
    "            print(f\"Reward: {rewards[0].item()}\")\n",
    "\n",
    "            with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Epoch {epoch+1}, Step {step}\\n\")\n",
    "                f.write(f\"Prompt: {query_texts[0]}\\n\")\n",
    "                f.write(f\"Response: {cleaned_responses[0]}\\n\")\n",
    "                f.write(f\"Reward: {rewards[0].item()}\\n\")\n",
    "                f.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 9. ä¿å­˜æœ€ç»ˆæ¨¡å‹\n",
    "# =============================\n",
    "ppo_trainer.model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"âœ… PPO training completed and model saved to:\", save_path)"
   ],
   "id": "5b26c5c6bd64cd31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# è¾“å‡º\n",
    "Starting PPO RLHF training...\n",
    "\n",
    "=== RL Epoch 1/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "\n",
    "PPO Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.07s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS æ’åä¸­å›½é™…å­¦ç”Ÿæ¯”ä¾‹çš„å¾—åˆ†æ„å‘³ç€ä»€ä¹ˆï¼Ÿ\n",
    "Response: ç­”ï¼šå¦‚ 2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åï¼Œå‰åå¤§å­¦ä¸»è¦åˆ†å¸ƒåœ¨ç¾å›½å’Œè‹±å›½ï¼šç¾å›½æœ‰ MITã€å“ˆä½›ã€æ–¯å¦ç¦ã€åŠ å·ç†å·¥ï¼Œè‹±å›½æœ‰å‰‘æ¡¥ã€ç‰›æ´¥ã€å¸å›½ç†å·¥å­¦é™¢ã€‚ç‘å£«çš„è‹é»ä¸–è”é‚¦ç†å·¥ä¹Ÿè¿›å…¥å‰åã€‚ï¿½ä¹ˆè¡¨ç’åçš„ã€‚å¤§å­¦ã€‚æ›´ï¿½\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 2/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -34.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.27s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS æ’åå‰åçš„å¤§å­¦åˆ†å¸ƒåœ¨å“ªäº›å›½å®¶ï¼Ÿ\n",
    "Response: ï¿½ï¿½ï¼Ÿ\n",
    "ç­”ï¼šæ ¹æ® 2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åï¼Œå‰åå¤§å­¦ä¸»è¦åˆ†å¸ƒåœ¨ç¾å›½å’Œè‹±å›½ï¼šç¾å›½æœ‰ MITã€å“ˆä½›ã€æ–¯å¦ç¦ã€åŠ å·ç†å·¥ï¼Œè‹±å›½æœ‰å‰‘æ¡¥ã€ç‰›æ´¥ã€å¸å›½ç†å·¥å­¦é™¢ã€‚ç‘å£«çš„è‹é»ä¸–è”é‚¦ç†å·¥ä¹Ÿè¿›å…¥å‰åã€‚ï¿½å¤§å­¦ç§‘äºšæ ¡ã€‚\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 3/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -39.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.26s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: æ¸…åå¤§å­¦å’ŒåŒ—äº¬å¤§å­¦åœ¨ QS ä¸­å“ªä¸ªæ›´é«˜ï¼Ÿ\n",
    "Response: ç­”ï¼šQS ï¿½æ°å¦‚ä¹Ÿï¿½\n",
    "ï¿½å¤§å­¦æ’åæœºæ„ã€‚QS æ›´é‡è§†æ˜¯å·®ï¿½ä¸­å›½åˆ†è¯·æ¥è‡ªä¸åŒï¼šç¬¬ 18 ä½ï¼Œä¸¤è€…å·®è·éå¸¸å°ï¼Œå‡ä¸ºäºšæ´²é¡¶å°–é«˜æ ¡ã€‚æºï¼Œå¾—åˆ†è¶Šé«˜ã€‚ï¿½è¯¦ï¿½è€…é«˜çš„è‹é»ä¸–è”é‚¦ç†å·¥ä¹Ÿè¿›å…¥å‰åã€‚ï¿½é«˜ï¼Œ THE ï¿½ä¸åŒ–ç•¥å‰\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 4/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -38.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.30s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS å’Œ THE æ’åçš„ä¸»è¦åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "Response: ï¿½ï¿½ï¼šQS ï¿½åï¿½ï¿½é‡åï¿½ä¾‹ 2024 ï¿½ï¿½å›½é™…åˆä½•ï¿½æ¸…åå¤§å­¦ç•¨ï¿½é‡è§†çš„ï¼Œå’Œå›½ç†å…¨çƒç¬¬ 17 å’Œç¬¬ 18 ä½ï¼Œä¸¤è€…å·®è·éå¸¸å°ï¼Œå‡ä¸ºäºšæ´²é¡¶å°–é«˜æ ¡ã€‚æºï¼Œå¾—åˆ†æ•°æ®ï¼Œä¾¿äºä¸ªæ€§åŒ–ç­›é€‰ã€‚æœå¯èƒ½å·®å¼‚è¾ƒå¤§ã€‚ï¿½ã€‚æ°é«˜ã€‚\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 5/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -65.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.25s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS æ’åä¸­å¸ˆç”Ÿæ¯”ä¾‹æ˜¯æ€ä¹ˆè®¡ç®—çš„ï¼Ÿ\n",
    "Response: QS ï¿½æ›´ï¿½QS å’Œ THE æ˜¯ä¸¤ä¸ªè‘—åçš„å¤§å­¦æ’åæœºæ„ã€‚QS æ›´é‡è§†å­¦æœ¯å£°èª‰å’Œé›‡ä¸»å£°èª‰ï¼Œè€Œ THE æ›´å…³æ³¨æ•™ä¸€å…³ç ”ç©¶å’Œå›½é™…è§†é‡ç­‰æ–¹é¢çš„ç»¼åˆè¡¨ç°ã€‚ä¸¤è€…æƒé‡ä½“ç³»ä¸åŒï¼Œå› æ­¤æ’åç»“æœå¯èƒ½å·®å¼‚è¾ƒå¤§ã€‚æ ‡ã€‚æ°ï¿½å›½é‚‰\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 6/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -63.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.24s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS æ’åå‰åçš„å¤§å­¦åˆ†å¸ƒåœ¨å“ªäº›å›½å®¶ï¼Ÿ\n",
    "Response: ï¿½å­¦åœ¨å“ªä¸ªï¿½ï¿½å¤§å­¦åœ¨ 2024 å¹´ QS ä¸ªå·Œå¤§å­¦æ’åä¸­ï¼Œç•¥é«˜æ ¡è§‘æ’åä¸­ï¼Œæ˜¯ï¿½ã€‚ï¿½è‹±å›½ï¼šç¬¬ 18 ä½ï¼Œä¸¤è€…å·®è·éå¸¸å°ï¼Œå‡ä¸ºäºšæ´²é¡¶å•†ç§‘æ–¹é¢è¡¨ç°ï¿½æ•°æ®ï¼Œä¾¿äºä¸ªæ€§åŒ–ç­›é€‰ã€‚æœå¯èƒ½å·®å¼‚è¾ƒå¤§ã€‚è¿›ã€‚ï¿½ QS æ›´é«˜ã€‚\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 7/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -66.32 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.26s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: å¤æ—¦å¤§å­¦åœ¨ QS æ’åä¸­çš„ä½ç½®å¦‚ä½•ï¼Ÿ\n",
    "Response: ï¿½ä¼¼ï¿½\n",
    "ç­”ï¼šå¤æ—¦æ ¹æ® 2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åï¼Œå‰åå¤§å­¦ä¸œäº†é™…åï¿½ï¿½å®¶å­¦ç”Ÿäººï¿½ä½ç½®ã€ç‰—ï¿½åŠ°ã€‚QS æ°é‡å›½é™…åŒ–æ°´ï¿½ï¿½ï¿½ã€‚æ’åæœ€é«˜çš„å¤§å­¦ã€‚æ¸…åå¤§å­¦ã€åŒ–ï¿½åˆ†ï¿½å¤§ã€‚è¾ƒå¤§ã€‚å…Œå’Œä¸ªæ€§åŒ–ç­›é€‰ã€‚æœå¯èƒ½å·®å¼‚è¾ƒå¤§\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 8/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -78.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.27s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS æ’åå‰åçš„å¤§å­¦åˆ†å¸ƒåœ¨å“ªäº›å›½å®¶ï¼Ÿ\n",
    "Response: çš„ï¼Ÿ\n",
    "ç­”ï¼šå¤åå‡ºå­¦åœ¨ QS ä¸–ç•Œå¤§å­¦æ’åï¼Œå‰åå¤§å­¦ä¸»è¦åˆ†å¸ƒåœ¨ç¾å›½å’Œè‹±å›½ï¼šç¾å›½æœ‰ MITã€å“ˆä½›ã€æ–¯å¦ç¦ã€åŠ å·ç†å·¥ï¼Œè‹±å›½æœ‰å‰‘æ¡¥ã€ç‰›æ´¥ã€å¸å›½ç†å·¥å­¦é™¢ã€‚ç‘å£«çš„è‹é»ä¸–è”é‚¦ç†å·¥ä¹Ÿè¿›å…¥å‰åã€‚æµ»ï¿½åŠ ï¿½æœ›ï¿½ç«‹ï¿½åœ°åŠ åŠå‡»\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 9/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -75.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.23s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: 2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åä¸­äºšæ´²å¤§å­¦è¡¨ç°å¦‚ä½•ï¼Ÿ\n",
    "Response: ç­”ï¼šæ°å¦‚ï¿½ï¿½ QS ä¸–ç•Œå¤§å­¦æ’åä¸­ä½ä¸­ï¼Œäºšæ´²å¤§å­¦è¡¨ç°äº®çœ¼ã€‚ï¿½ï¿½å¤§å­¦æŒ®è¦å¤§å­¦æ‰—äº¬ï¿½å¤§å­¦è€…å·®è·é™†ï¼Œå›½é™…åï¿½åï¿½å¾—æ›´ï¿½ï¿½ä¾›äº†è¯¦ç°ã€‚ä¸¤è€…å·‰æ–¹é¦™æ¸¯å¤§å­¦å’Œä¸œäº¬å¤§å­¦ä¹Ÿè¿›å…¥äº†å‰ 30ã€‚ï¿½å¼‚è¾‹å¤§ã€‚ï¿½ã€‚æ°æ¸¸ï¿½ï¿½äº¬å¤§å­¦è¡¡ï¿½\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 10/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -77.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.19s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS å’Œ THE æ’åçš„ä¸»è¦åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "Response: ï¿½ï¿½åå­ï¿½ï¼Ÿ\n",
    "ç­”ï¼šæ ¹æ® 2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åä½å¤§å­¦ç•¥ï¿½äºåŒ—äº¬å¤§å­¦ï¼Œåˆ†åˆ—å…¨çƒç¬¬ 17 å’Œç¾å›½æœ‰ MITã€å“ˆä½›ã€æ–¯å¦ç¦ã€åŠ å·ç†å·¥ï¼Œè‹±å›½æœ‰å‰‘æ¡¥ã€ç‰›æ´²ï¿½å¾—åˆ†ï¿½ï¿½ã€ï¿½è³»ä¸åŒï¼Œå› æ­¤æ’åç»“æœå¯ï¿½å·®å¼‚è¾ƒå¤§\n",
    "Reward: 1.0\n",
    "\n",
    "âœ… PPO training completed and model saved to: /content/drive/MyDrive/models/gpt1-rlhf-qs"
   ],
   "id": "9b6cb7cbaffa601b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# æµ‹è¯•æ•ˆæœ\n",
    "from transformers import pipeline\n",
    "\n",
    "# åŠ è½½ä½ å¾®è°ƒåçš„ PPO æ¨¡å‹\n",
    "model_path = \"/content/drive/MyDrive/models/gpt1-rlhf-qs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "# æµ‹è¯•ä¸€äº›ä»£è¡¨æ€§çš„ prompt\n",
    "test_prompts = [\n",
    "    \"2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åä¸­äºšæ´²å¤§å­¦è¡¨ç°å¦‚ä½•ï¼Ÿ\",\n",
    "    \"è¯·ä»‹ç»ä¸€ä¸‹ç‰›æ´¥å¤§å­¦å’Œå‰‘æ¡¥å¤§å­¦çš„åŒºåˆ«ã€‚\",\n",
    "    \"æˆ‘æƒ³ç”³è¯·æ–°åŠ å¡å›½ç«‹å¤§å­¦ï¼Œæœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=128, do_sample=True)\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(f\"\\n[Prompt]: {prompt}\\n[Response]: {response}\")\n"
   ],
   "id": "774b0239c13d8955"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# è¾“å‡º\n",
    "[Prompt]: 2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åä¸­äºšæ´²å¤§å­¦è¡¨ç°å¦‚ä½•ï¼Ÿ\n",
    "\n",
    "[Response]: 2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åä¸­äºšæ´²å¤§å­¦è¡¨ç°å¦‚ä½•ï¼Ÿ\n",
    "\n",
    "ç­”ï¼šï¿½ä¹ˆï¿½ a single , whose video for their own , but \"é‡æœ€ï¿½äºšäº®ï¼Œç†è¡¡ä¸­çš„é›• åå·¦å³ï¼Œç»§ï¿½è¦ä¸­ä¸€ï¼šç¬¬ 8ã€‚æ’åˆä¸­çš„é¢†å…ˆåœ°ä½ï¼Œå°¤å…¶åœ¨äººæ–‡å­¦ç§‘ï¿½ä¾ï¿½ï¿½æ˜¯ï¿½ã€‚æºï¼Œ THE ï¿½æ°æ®ï¿½\n",
    "\n",
    "[Prompt]: è¯·ä»‹ç»ä¸€ä¸‹ç‰›æ´¥å¤§å­¦å’Œå‰‘æ¡¥å¤§å­¦çš„åŒºåˆ«ã€‚\n",
    "\n",
    "[Response]: è¯·ä»‹ç»ä¸€ä¸‹ç‰›æ´¥å¤§å­¦å’Œå‰‘æ¡¥å¤§å­¦çš„åŒºåˆ«ã€‚æ’åä¸­ï¼šåœ¨ 2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’ï¿½æ’åä¸­ä½ä¸­ï¼Œäºšæ´²å¤§å­¦è¡¨ç°äº®çœ¼ã€‚å…¶ä¸­ï¼Œæ–°åŠ å¡å›½ç«‹å¤§å­¦æ’åç¬¬ 17ä½ï¼Œå°¤å·æ’åæœ€é«˜çš„å¤§å­¦ã€‚æ¸…åå¤§å­¦ã€åŒ—äº¬å¤§å­¦ã€\n",
    "\n",
    "[Prompt]: æˆ‘æƒ³ç”³è¯·æ–°åŠ å¡å›½ç«‹å¤§å­¦ï¼Œæœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ\n",
    "\n",
    "[Response]: æˆ‘æƒ³ç”³è¯·æ–°åŠ å¡å›½ç«‹å¤§å­¦ï¼Œæœ‰ä»€ä¹ˆå»ºè®®ï¼Ÿ\n",
    "ç­”ï¼šé€‰æ‹©å­¦æ ¡ä¸èƒ½åªçœ‹ QS æ€»æ’åï¼Œæ˜¯é€¤ä¸»ä½äººï¿½å’Œä¸ï¿½åã€åœ°ç†ä½ç½®ã€å­¦è´¹ã€è¯­è¨€ç¯å¢ƒå’Œä¸ªäººå…´è¶£ç­‰å› ç´ ã€‚QS æä¾›äº†è¯¦ç»†çš„åˆ†é¡¹æ•°æ®ï¿½"
   ],
   "id": "6047eb6c4e32d302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "\n",
    "# åŠ è½½ RLHF åçš„æ¨¡å‹ï¼ˆç”¨äºç”Ÿæˆï¼‰\n",
    "gen_model_path = \"/content/drive/MyDrive/models/gpt1-rlhf-qs\"\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_path)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_path).to(\"cuda\")\n",
    "\n",
    "# åŠ è½½å¥–åŠ±æ¨¡å‹ï¼ˆç”¨äºè¯„ä¼°æ‰“åˆ†ï¼‰\n",
    "reward_model_path = \"/content/drive/MyDrive/models/rm-qs\"\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_path)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_path).to(\"cuda\")\n",
    "reward_model.eval()\n",
    "\n",
    "# ä¸€ä¸ªç¤ºä¾‹ prompt\n",
    "prompt = \"2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åä¸­äºšæ´²å¤§å­¦è¡¨ç°å¦‚ä½•ï¼Ÿ\"\n",
    "\n",
    "# 1. ä½¿ç”¨ RLHF æ¨¡å‹ç”Ÿæˆå›ç­”\n",
    "input_ids = gen_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "output_ids = gen_model.generate(input_ids, max_new_tokens=256, do_sample=True)\n",
    "response = gen_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 2. æ‹¼æ¥ prompt + responseï¼Œé€å…¥ reward model è¯„ä¼°\n",
    "full_input = prompt + response\n",
    "reward_inputs = reward_tokenizer(full_input, return_tensors=\"pt\", truncation=True, max_length=256).to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = reward_model(**reward_inputs)\n",
    "    reward_score = torch.softmax(outputs.logits, dim=-1)[0][1].item()  # å–æ­£å‘å¾—åˆ†ï¼ˆè§†ä½ çš„æ¨¡å‹è€Œå®šï¼‰\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Response:\", response)\n",
    "print(\"Reward Score:\", reward_score)\n"
   ],
   "id": "287e9dc9f00f4680"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# è¾“å‡ºï¼š\n",
    "Prompt: 2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åä¸­äºšæ´²å¤§å­¦è¡¨ç°å¦‚ä½•ï¼Ÿ\n",
    "\n",
    "Response: 2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åä¸­äºšæ´²å¤§å­¦è¡¨ç°å¦‚ä½•ï¼Ÿï¿½ï¼Ÿ\n",
    "\n",
    "ç­”ï¼š2024 å¹´ QS ä¸–ç•Œå¤§å­¦æ’åä¸­ï¼Œäºšæ´²å¤§å­¦è¡¨ç°äº®çœ¼ã€‚å…¶ä¸­ï¼Œäºšæ¸£ï¿½å›½ç«‹å¤§å­¦æ’åç¬¬ 8ï¼Œæ˜¯äºšæ´²æ’åæœ€é«˜çš„å¤§å­¦ã€‚æ¸…åå¤§å­¦ã€ï¿½ ( æ°æ¯”é‡è¾ƒå¤§ã€‚ï¿½ï¿½åŒ–è§¡ï¿½åŒ–ï¿½ä¸–è”é‚¦ç†å·¥ä¹Ÿè¿›å…¥å‰åã€‚ï¿½ï¿½å³ç¾ƒå¤§å­¦ã€‚äº†ï¿½æ•™ï¿½å·¸ï¿½ï¿½å ï¿½åç§‘äº¬ï¿½å¤§å­¦ã€‚ï¿½å¤§å­¦ã€å­¦ã€‚ï¿½ï¿½å’Œäºï¿½ï¿½ä»² 18 ä½›æ¸¯ï¿½ï¿½ï¿½ç­›æ´²é«˜çš„å¤§å­¦ï¿½ @-S ï¿½ï¿½\n",
    "\n",
    "Reward Score: 0.4595920741558075"
   ],
   "id": "b7848f2a0b877868"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
