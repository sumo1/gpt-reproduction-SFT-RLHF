{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# 📌 1. 导入库\n",
    "import torch\n",
    "from transformers import (\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  #这里使用了colab中的google drive来保存信息到云盘。\n",
    "\n",
    "# 📌 2. 检查 GPU\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n",
    "\n",
    "# 📌 3. 加载 wikitext 数据集\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# 📌 4. 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 必须设置 pad_token\n",
    "\n",
    "# 📌 5. Tokenize & 拼接数据为长序列\n",
    "block_size = 128\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"])\n",
    "\n",
    "def group_texts(examples):\n",
    "    # 拼接所有 token\n",
    "    joined = sum(examples[\"input_ids\"], [])\n",
    "    total_length = len(joined)\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        \"input_ids\": [joined[i:i+block_size] for i in range(0, total_length, block_size)],\n",
    "        \"attention_mask\": [ [1]*block_size ] * (total_length // block_size)\n",
    "    }\n",
    "    return result\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "lm_dataset = tokenized.map(group_texts, batched=True)\n",
    "\n",
    "# 📌 6. 构建模型\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=512,\n",
    "    n_ctx=512,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12,\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# 📌 7. 训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/models/gpt1-wikitext-model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_dir=\"/content/drive/MyDrive/models/logs\",\n",
    "    save_total_limit=2,\n",
    "    fp16=True  # ✅ 混合精度\n",
    ")\n",
    "\n",
    "# 📌 8. 训练器准备\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# 📌 9. 开始训练！\n",
    "trainer.train()\n",
    "\n",
    "# 📌 10. 保存模型\n",
    "trainer.save_model(\"/content/drive/MyDrive/models/gpt1-wikitext-final\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/models/gpt1-wikitext-final\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 查看训练曲线\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logs = trainer.state.log_history\n",
    "steps = [log[\"step\"] for log in logs if \"loss\" in log]\n",
    "losses = [log[\"loss\"] for log in logs if \"loss\" in log]\n",
    "\n",
    "plt.plot(steps, losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"GPT-1 Training Loss\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "854f2f8c7c086acf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 查询训练效果\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, pipeline\n",
    "\n",
    "# 加载你训练好的模型和 tokenizer\n",
    "model_path = \"/content/drive/MyDrive/models/gpt1-wikitext-final\"  # 你实际保存的位置\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.to(\"cuda\").eval()  # 加上 eval() 和 CUDA\n",
    "\n",
    "# 创建生成 pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# 试试你的 prompt\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "outputs = generator(\n",
    "    prompt,\n",
    "    max_length=100,          # 最长生成长度\n",
    "    do_sample=True,          # 使用采样\n",
    "    top_k=50,                # 采样 top-k\n",
    "    top_p=0.95,              # nucleus sampling\n",
    "    temperature=1.0,         # 控制多样性\n",
    "    num_return_sequences=1   # 返回一个结果\n",
    ")\n",
    "\n",
    "print(\"=== Generated Text ===\")\n",
    "print(outputs[0][\"generated_text\"])"
   ],
   "id": "2b0fc82b44175cf0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 结果输出：\n",
    "=== Generated Text ===\n",
    "\n",
    "The future of artificial intelligence on the United States, and the entire city was the first first series. He was built by the game at $ 6 in the 19th World War after the 2006, the series was released in the 19 – 1 January 2007. When the same year he had first year as well as well as part of all @-@ single as a result of the New York, although the season was played in September 2009. It was released, and the song was established by the year"
   ],
   "id": "20fee1d9b3030c62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 基于qs_sft_zh.jsonl的数据进行SFT\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "import torch\n",
    "\n",
    "# 路径\n",
    "data_path = \"/content/drive/MyDrive/data/qs_sft_zh.jsonl\"\n",
    "model_path = \"/content/drive/MyDrive/models/gpt1-wikitext-final\"\n",
    "save_path = \"/content/drive/MyDrive/models/gpt1-sft-qs\"\n",
    "\n",
    "# 📌 1. 加载原始 JSONL 数据并切分验证集\n",
    "dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# 📌 2. 加载 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 📌 3. Tokenize 数据\n",
    "def format_prompt(example):\n",
    "    full_text = f\"问：{example['prompt']}\\n答：{example['response']}\"\n",
    "    return {\"text\": full_text}\n",
    "\n",
    "train_dataset = train_dataset.map(format_prompt)\n",
    "eval_dataset = eval_dataset.map(format_prompt)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"])\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=[\"prompt\", \"response\", \"text\"])\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True, remove_columns=[\"prompt\", \"response\", \"text\"])\n",
    "\n",
    "\n",
    "# 📌 4. 加载模型\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# 📌 5. 训练参数（开启验证）\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    eval_strategy=\"steps\",   # ✅ 每 N 步评估一次\n",
    "    eval_steps=100,\n",
    "    logging_dir=f\"{save_path}/logs\",\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# 📌 6. Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "# 📌 7. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # ✅ 添加验证集\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# 📌 8. 开始训练\n",
    "trainer.train()\n",
    "\n",
    "# 📌 9. 保存模型\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ],
   "id": "d86335553402ff66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 验证微调结果\n",
    "import torch\n",
    "\n",
    "def ask_model(question, max_new_tokens=128):\n",
    "    prompt = f\"用户：{question}\\n助手：\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False  # greedy decoding\n",
    "        )\n",
    "\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = full_output.split(\"助手：\", 1)[-1].strip()\n",
    "    return answer\n",
    "\n",
    "print(ask_model(\"2024 年 QS 世界大学排名中亚洲大学表现如何？\"))\n",
    "print(ask_model(\"QS 和 THE 排名的主要区别是什么？\"))\n",
    "print(ask_model(\"如何使用 QS 排名选择适合自己的学校？\"))"
   ],
   "id": "fdf422fd310ca768"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 输出\n",
    "排名中，清华大学略高于北京大学，分列全球第 17 和第 18 位，两者差距非常小，均为亚洲顶尖高校。源，得分越高。�便于东京大学也\n",
    "QS 是两个著名的大学排名机构。QS 更重视学术声誉和雇主声誉，而 THE 更关注教学、研究和国际视野等方面的综合表现。两者权重体系不�\n",
    "逮如指标是通过统计位，还应关注学科排名、地理位置、学费、语言环境和个人兴趣等因素。QS 提供了详细的分项数据，便于个�"
   ],
   "id": "1fef175f6071e7d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 准备SFT之后的强化学习，提升性能，基于RLHF\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "from transformers import AutoTokenizer, GPT2Model, GPT2Config, get_scheduler\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# === 1. 参数设置 ===\n",
    "model_name_or_path = \"/content/drive/MyDrive/models/gpt1-sft-qs\"\n",
    "data_path = \"/content/drive/MyDrive/data/qs_ranking_rm_data.jsonl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_length = 512\n",
    "batch_size = 2\n",
    "num_epochs = 8\n",
    "lr = 5e-5\n",
    "\n",
    "# === 2. 数据集 ===\n",
    "class PairwiseRMData(Dataset):\n",
    "    def __init__(self, path, tokenizer):\n",
    "        self.data = []\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                #print(line)\n",
    "                item = json.loads(line)\n",
    "                self.data.append(item)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.data[idx]\n",
    "        prompt = d[\"prompt\"]\n",
    "        chosen = prompt + d[\"chosen\"]\n",
    "        rejected = prompt + d[\"rejected\"]\n",
    "        chosen_ids = self.tokenizer(chosen, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "        rejected_ids = self.tokenizer(rejected, return_tensors=\"pt\", truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "        return {\n",
    "            \"chosen_input_ids\": chosen_ids[\"input_ids\"].squeeze(0),\n",
    "            \"chosen_attention_mask\": chosen_ids[\"attention_mask\"].squeeze(0),\n",
    "            \"rejected_input_ids\": rejected_ids[\"input_ids\"].squeeze(0),\n",
    "            \"rejected_attention_mask\": rejected_ids[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# === 3. 模型结构 ===\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path):\n",
    "        super().__init__()\n",
    "        config = GPT2Config.from_pretrained(model_name_or_path)\n",
    "        config.pad_token_id = config.eos_token_id\n",
    "        self.gpt = GPT2Model.from_pretrained(model_name_or_path, config=config)\n",
    "        self.value_head = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs.last_hidden_state  # [B, L, H]\n",
    "        rewards = self.value_head(last_hidden).squeeze(-1)  # [B, L]\n",
    "        lengths = attention_mask.sum(dim=1) - 1\n",
    "        last_rewards = rewards[range(rewards.size(0)), lengths]\n",
    "        return last_rewards\n",
    "\n",
    "# === 4. 初始化 ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dataset = PairwiseRMData(data_path, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = RewardModel(model_name_or_path).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * num_epochs)\n",
    "\n",
    "# === 5. 训练循环 ===\n",
    "loss_fn = nn.MarginRankingLoss(margin=0.5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch in pbar:\n",
    "        chosen_ids = batch[\"chosen_input_ids\"].to(device)\n",
    "        chosen_mask = batch[\"chosen_attention_mask\"].to(device)\n",
    "        rejected_ids = batch[\"rejected_input_ids\"].to(device)\n",
    "        rejected_mask = batch[\"rejected_attention_mask\"].to(device)\n",
    "\n",
    "        chosen_rewards = model(chosen_ids, chosen_mask)\n",
    "        rejected_rewards = model(rejected_ids, rejected_mask)\n",
    "\n",
    "        loss = loss_fn(chosen_rewards, rejected_rewards, torch.ones_like(chosen_rewards).to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "# === 6. 保存模型 ===\n",
    "save_path = \"/content/drive/MyDrive/models/rm-qs\"\n",
    "model.gpt.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"✅ 奖励模型保存完成！\")"
   ],
   "id": "4981325207cad446"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 输出\n",
    "Epoch 1: 100%|██████████| 140/140 [00:32<00:00,  4.30it/s, loss=0]\n",
    "\n",
    "Epoch 2: 100%|██████████| 140/140 [00:32<00:00,  4.33it/s, loss=0]\n",
    "\n",
    "Epoch 3: 100%|██████████| 140/140 [00:32<00:00,  4.35it/s, loss=0]\n",
    "\n",
    "Epoch 4: 100%|██████████| 140/140 [00:32<00:00,  4.34it/s, loss=0]\n",
    "\n",
    "Epoch 5: 100%|██████████| 140/140 [00:32<00:00,  4.33it/s, loss=0]\n",
    "\n",
    "Epoch 6: 100%|██████████| 140/140 [00:32<00:00,  4.34it/s, loss=0]\n",
    "\n",
    "Epoch 7: 100%|██████████| 140/140 [00:32<00:00,  4.34it/s, loss=0]\n",
    "\n",
    "Epoch 8: 100%|██████████| 140/140 [00:32<00:00,  4.34it/s, loss=0]\n",
    "\n",
    "✅ 奖励模型保存完成！"
   ],
   "id": "4703705ff7f42072"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# 加载模型和 tokenizer\n",
    "reward_model_path = \"/content/drive/MyDrive/models/rm-qs\"\n",
    "\n",
    "rm_tokenizer = AutoTokenizer.from_pretrained(reward_model_path)\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained(reward_model_path)\n",
    "rm_model.eval().cuda()  # 如果有 GPU 的话\n",
    "\n",
    "# 输入测试对\n",
    "prompt = \"请介绍清华大学。\"\n",
    "response = \"清华大学是中国顶尖的综合性大学，位于北京市。\"\n",
    "\n",
    "# 构造模型输入\n",
    "text = f\"Prompt: {prompt}\\nResponse: {response}\"  # 确保格式和训练时一致\n",
    "inputs = rm_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(rm_model.device)\n",
    "\n",
    "# 前向推理得到 reward 分数\n",
    "logits = rm_model(**inputs).logits\n",
    "reward = logits.squeeze()\n",
    "if reward.numel() > 1:\n",
    "    reward = reward[0]\n",
    "print(\"Reward score:\", reward.item())"
   ],
   "id": "5b1c3dd9c45407f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 输出\n",
    "Reward score: 0.35942158102989197\n",
    "\n"
   ],
   "id": "bf25d8c12dbe926c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 基于上面的SFT和奖励模型进行PPO RLHF\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, GPT2Model, GPT2Config\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from torch import nn\n",
    "\n",
    "# =============================\n",
    "# 1. 路径设置\n",
    "# =============================\n",
    "sft_model_path = \"/content/drive/MyDrive/models/gpt1-sft-qs\"\n",
    "reward_model_path = \"/content/drive/MyDrive/models/rm-qs\"\n",
    "save_path = \"/content/drive/MyDrive/models/gpt1-rlhf-qs\"\n",
    "prompts_file = \"/content/drive/MyDrive/data/qs_sft_zh.jsonl\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 2. 自定义 Dataset\n",
    "# =============================\n",
    "class PromptOnlyDataset(TorchDataset):\n",
    "    def __init__(self, jsonl_path):\n",
    "        self.data = []\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                self.data.append(item[\"prompt\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"prompt\": self.data[idx]}\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 3. 加载模型和 tokenizer\n",
    "# =============================\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(sft_model_path).to(device)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(sft_model_path).to(device)\n",
    "ref_model.eval()\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 4. 定义数据集和 collator\n",
    "# =============================\n",
    "prompts_dataset = PromptOnlyDataset(prompts_file)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    prompts = [item[\"prompt\"] for item in batch]\n",
    "    tokenized = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
    "    tokenized[\"input_ids\"] = tokenized[\"input_ids\"].to(device)\n",
    "    tokenized[\"attention_mask\"] = tokenized[\"attention_mask\"].to(device)\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 5. PPO 配置和 Trainer\n",
    "# =============================\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=8,\n",
    "    mini_batch_size=4,\n",
    "    log_with=\"tensorboard\",\n",
    "    project_kwargs={\"logging_dir\": \"/content/drive/MyDrive/models/tensorboard_logs\"}\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=prompts_dataset,\n",
    "    data_collator=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 6. 加载奖励模型\n",
    "# =============================\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, model_name_or_path):\n",
    "        super().__init__()\n",
    "        config = GPT2Config.from_pretrained(model_name_or_path)\n",
    "        config.pad_token_id = config.eos_token_id\n",
    "        self.gpt = GPT2Model.from_pretrained(model_name_or_path, config=config)\n",
    "        self.value_head = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.gpt(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs.last_hidden_state  # [B, L, H]\n",
    "        rewards = self.value_head(last_hidden).squeeze(-1)  # [B, L]\n",
    "        lengths = attention_mask.sum(dim=1) - 1\n",
    "        last_rewards = rewards[range(rewards.size(0)), lengths]\n",
    "        return last_rewards\n",
    "\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_path)\n",
    "reward_tokenizer.pad_token = reward_tokenizer.eos_token\n",
    "reward_model = RewardModel(reward_model_path).to(device)\n",
    "reward_model.eval()\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 7. 奖励函数（调用 reward model）\n",
    "# =============================\n",
    "def reward_fn(responses, prompts):\n",
    "    texts = [p + r for p, r in zip(prompts, responses)]\n",
    "    tokenized = reward_tokenizer(\n",
    "        texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        rewards = reward_model(\n",
    "            input_ids=tokenized[\"input_ids\"],\n",
    "            attention_mask=tokenized[\"attention_mask\"]\n",
    "        )\n",
    "    return list(rewards)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 8. PPO 强化训练主循环\n",
    "# =============================\n",
    "num_rl_epochs = 10\n",
    "log_file = \"/content/drive/MyDrive/models/ppo_rlhf_log.txt\"\n",
    "\n",
    "print(\"Starting PPO RLHF training...\")\n",
    "\n",
    "for epoch in range(num_rl_epochs):\n",
    "    print(f\"=== RL Epoch {epoch+1}/{num_rl_epochs} ===\")\n",
    "\n",
    "    for step, batch in enumerate(tqdm(ppo_trainer.dataloader, desc=\"PPO Training\")):\n",
    "        queries = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        # === 模型生成回复 ===\n",
    "        responses = model.generate(\n",
    "            input_ids=queries,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        query_texts = tokenizer.batch_decode(queries, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        response_texts = tokenizer.batch_decode(responses, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "        # === 去除重复 prompt ===\n",
    "        cleaned_responses = []\n",
    "        for q, r in zip(query_texts, response_texts):\n",
    "            if r.startswith(q):\n",
    "                cleaned_responses.append(r[len(q):].strip())\n",
    "            else:\n",
    "                cleaned_responses.append(r.strip())\n",
    "\n",
    "        # === 计算奖励 ===\n",
    "        rewards = reward_fn(cleaned_responses, query_texts)\n",
    "\n",
    "        # === Tokenize 再次用于 PPOTrainer ===\n",
    "        query_ids = [q.to(device) for q in queries]\n",
    "        response_ids = [\n",
    "            tokenizer(r, return_tensors=\"pt\", truncation=True, max_length=256).input_ids[0].to(device)\n",
    "            for r in cleaned_responses\n",
    "        ]\n",
    "\n",
    "        stats = ppo_trainer.step(query_ids, response_ids, rewards)\n",
    "\n",
    "        if step % 1 == 0:\n",
    "            print(f\"\\nStep {step}\")\n",
    "            print(f\"Prompt: {query_texts[0][:50]}\")\n",
    "            print(f\"Response: {cleaned_responses[0][:100]}\")\n",
    "            print(f\"Reward: {rewards[0].item()}\")\n",
    "\n",
    "            with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(f\"Epoch {epoch+1}, Step {step}\\n\")\n",
    "                f.write(f\"Prompt: {query_texts[0]}\\n\")\n",
    "                f.write(f\"Response: {cleaned_responses[0]}\\n\")\n",
    "                f.write(f\"Reward: {rewards[0].item()}\\n\")\n",
    "                f.write(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 9. 保存最终模型\n",
    "# =============================\n",
    "ppo_trainer.model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(\"✅ PPO training completed and model saved to:\", save_path)"
   ],
   "id": "5b26c5c6bd64cd31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 输出\n",
    "Starting PPO RLHF training...\n",
    "\n",
    "=== RL Epoch 1/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "\n",
    "PPO Training: 100%|██████████| 1/1 [00:05<00:00,  5.07s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS 排名中国际学生比例的得分意味着什么？\n",
    "Response: 答：如 2024 年 QS 世界大学排名，前十大学主要分布在美国和英国：美国有 MIT、哈佛、斯坦福、加州理工，英国有剑桥、牛津、帝国理工学院。瑞士的苏黎世联邦理工也进入前十。�么表玒吁的。大学。更�\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 2/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -34.77 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS 排名前十的大学分布在哪些国家？\n",
    "Response: ��？\n",
    "答：根据 2024 年 QS 世界大学排名，前十大学主要分布在美国和英国：美国有 MIT、哈佛、斯坦福、加州理工，英国有剑桥、牛津、帝国理工学院。瑞士的苏黎世联邦理工也进入前十。�大学科亚校。\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 3/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -39.49 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|██████████| 1/1 [00:05<00:00,  5.26s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: 清华大学和北京大学在 QS 中哪个更高？\n",
    "Response: 答：QS �掰如也�\n",
    "�大学排名机构。QS 更重视是差�中国分请来自不同：第 18 位，两者差距非常小，均为亚洲顶尖高校。源，得分越高。�详�者高的苏黎世联邦理工也进入前十。�高， THE �与化略前\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 4/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -38.43 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|██████████| 1/1 [00:05<00:00,  5.30s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS 和 THE 排名的主要区别是什么？\n",
    "Response: ��：QS �名��重华�例 2024 ��国际午何�清华大学畨�重视的，和国理全球第 17 和第 18 位，两者差距非常小，均为亚洲顶尖高校。源，得分数据，便于个性化筛选。果可能差异较大。�。掰高。\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 5/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -65.09 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|██████████| 1/1 [00:05<00:00,  5.25s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS 排名中师生比例是怎么计算的？\n",
    "Response: QS �更�QS 和 THE 是两个著名的大学排名机构。QS 更重视学术声誉和雇主声誉，而 THE 更关注教一关研究和国际视野等方面的综合表现。两者权重体系不同，因此排名结果可能差异较大。标。掰�国邉\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 6/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -63.05 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|██████████| 1/1 [00:05<00:00,  5.24s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS 排名前十的大学分布在哪些国家？\n",
    "Response: �学在哪个��大学在 2024 年 QS 个巌大学排名中，略高校觑排名中，是�。�英国：第 18 位，两者差距非常小，均为亚洲顶商科方面表现�数据，便于个性化筛选。果可能差异较大。进。� QS 更高。\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 7/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -66.32 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|██████████| 1/1 [00:05<00:00,  5.26s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: 复旦大学在 QS 排名中的位置如何？\n",
    "Response: �似�\n",
    "答：复旦根据 2024 年 QS 世界大学排名，前十大学东了际华��家学生人�位置、牗�劰。QS 掰量国际化水���。排名最高的大学。清华大学、化�分�大。较大。兌和个性化筛选。果可能差异较大\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 8/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -78.59 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|██████████| 1/1 [00:05<00:00,  5.27s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS 排名前十的大学分布在哪些国家？\n",
    "Response: 的？\n",
    "答：复十出学在 QS 世界大学排名，前十大学主要分布在美国和英国：美国有 MIT、哈佛、斯坦福、加州理工，英国有剑桥、牛津、帝国理工学院。瑞士的苏黎世联邦理工也进入前十。浻�加�望�立�地加劁击\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 9/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -75.80 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|██████████| 1/1 [00:05<00:00,  5.23s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: 2024 年 QS 世界大学排名中亚洲大学表现如何？\n",
    "Response: 答：掰如�� QS 世界大学排名中位中，亚洲大学表现亮眼。��大学挮要大学扗京�大学者差距陆，国际华�后�得更��供了详现。两者巉方香港大学和东京大学也进入了前 30。�异辋大。�。掰游��京大学衡�\n",
    "Reward: 1.0\n",
    "\n",
    "=== RL Epoch 10/10 ===\n",
    "\n",
    "PPO Training:   0%|          | 0/1 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
    "/usr/local/lib/python3.11/dist-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -77.76 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.\n",
    "  warnings.warn(\n",
    "\n",
    "PPO Training: 100%|██████████| 1/1 [00:05<00:00,  5.19s/it]\n",
    "\n",
    "Step 0\n",
    "Prompt: QS 和 THE 排名的主要区别是什么？\n",
    "Response: ��名卭�？\n",
    "答：根据 2024 年 QS 世界大学排名低大学略�于北京大学，分列全球第 17 和美国有 MIT、哈佛、斯坦福、加州理工，英国有剑桥、牛洲�得分��、�賻不同，因此排名结果可�差异较大\n",
    "Reward: 1.0\n",
    "\n",
    "✅ PPO training completed and model saved to: /content/drive/MyDrive/models/gpt1-rlhf-qs"
   ],
   "id": "9b6cb7cbaffa601b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 测试效果\n",
    "from transformers import pipeline\n",
    "\n",
    "# 加载你微调后的 PPO 模型\n",
    "model_path = \"/content/drive/MyDrive/models/gpt1-rlhf-qs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "# 测试一些代表性的 prompt\n",
    "test_prompts = [\n",
    "    \"2024 年 QS 世界大学排名中亚洲大学表现如何？\",\n",
    "    \"请介绍一下牛津大学和剑桥大学的区别。\",\n",
    "    \"我想申请新加坡国立大学，有什么建议？\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    output_ids = model.generate(input_ids, max_new_tokens=128, do_sample=True)\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(f\"\\n[Prompt]: {prompt}\\n[Response]: {response}\")\n"
   ],
   "id": "774b0239c13d8955"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 输出\n",
    "[Prompt]: 2024 年 QS 世界大学排名中亚洲大学表现如何？\n",
    "\n",
    "[Response]: 2024 年 QS 世界大学排名中亚洲大学表现如何？\n",
    "\n",
    "答：�么� a single , whose video for their own , but \"重最�亚亮，理衡中的雕 名左右，继�要中一：第 8。排合中的领先地位，尤其在人文学科�侎��是�。源， THE �掰据�\n",
    "\n",
    "[Prompt]: 请介绍一下牛津大学和剑桥大学的区别。\n",
    "\n",
    "[Response]: 请介绍一下牛津大学和剑桥大学的区别。排名中：在 2024 年 QS 世界大学排�排名中位中，亚洲大学表现亮眼。其中，新加坡国立大学排名第 17位，尤州排名最高的大学。清华大学、北京大学、\n",
    "\n",
    "[Prompt]: 我想申请新加坡国立大学，有什么建议？\n",
    "\n",
    "[Response]: 我想申请新加坡国立大学，有什么建议？\n",
    "答：选择学校不能只看 QS 总排名，是逤主位人�和与�名、地理位置、学费、语言环境和个人兴趣等因素。QS 提供了详细的分项数据�"
   ],
   "id": "6047eb6c4e32d302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "\n",
    "# 加载 RLHF 后的模型（用于生成）\n",
    "gen_model_path = \"/content/drive/MyDrive/models/gpt1-rlhf-qs\"\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_path)\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(gen_model_path).to(\"cuda\")\n",
    "\n",
    "# 加载奖励模型（用于评估打分）\n",
    "reward_model_path = \"/content/drive/MyDrive/models/rm-qs\"\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_path)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_path).to(\"cuda\")\n",
    "reward_model.eval()\n",
    "\n",
    "# 一个示例 prompt\n",
    "prompt = \"2024 年 QS 世界大学排名中亚洲大学表现如何？\"\n",
    "\n",
    "# 1. 使用 RLHF 模型生成回答\n",
    "input_ids = gen_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "output_ids = gen_model.generate(input_ids, max_new_tokens=256, do_sample=True)\n",
    "response = gen_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 2. 拼接 prompt + response，送入 reward model 评估\n",
    "full_input = prompt + response\n",
    "reward_inputs = reward_tokenizer(full_input, return_tensors=\"pt\", truncation=True, max_length=256).to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = reward_model(**reward_inputs)\n",
    "    reward_score = torch.softmax(outputs.logits, dim=-1)[0][1].item()  # 取正向得分（视你的模型而定）\n",
    "\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Response:\", response)\n",
    "print(\"Reward Score:\", reward_score)\n"
   ],
   "id": "287e9dc9f00f4680"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 输出：\n",
    "Prompt: 2024 年 QS 世界大学排名中亚洲大学表现如何？\n",
    "\n",
    "Response: 2024 年 QS 世界大学排名中亚洲大学表现如何？�？\n",
    "\n",
    "答：2024 年 QS 世界大学排名中，亚洲大学表现亮眼。其中，亚渣�国立大学排名第 8，是亚洲排名最高的大学。清华大学、� ( 掰比重较大。��化觡�化�世联邦理工也进入前十。��右羃大学。了�教�巸��占�名科京�大学。�大学、学。��和于��仲 18 佛港���筛洲高的大学� @-S ��\n",
    "\n",
    "Reward Score: 0.4595920741558075"
   ],
   "id": "b7848f2a0b877868"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
